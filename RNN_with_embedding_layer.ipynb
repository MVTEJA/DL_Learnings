{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "DzPKiTrGuBDF"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "\n",
        "docs = ['let’s win this',\n",
        "'team spirit',\n",
        "'cheer loud and proud',\n",
        "'champions never give up',\n",
        "'victory for the brave',\n",
        "'rohit rohit',\n",
        "'yuvi yuvi',\n",
        "'boom boom bumrah',\n",
        "'india on top',\n",
        "'unstoppable force'\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer(oov_token='<nothing>')"
      ],
      "metadata": {
        "id": "swQ6kZtcy4sy"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.fit_on_texts(docs)"
      ],
      "metadata": {
        "id": "26CtD1TEy4uC"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.word_index"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JHmT9C4Cy4xd",
        "outputId": "1066cded-dbbf-4dda-c7cf-dfb0208285b9"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'<nothing>': 1,\n",
              " 'rohit': 2,\n",
              " 'yuvi': 3,\n",
              " 'boom': 4,\n",
              " 'let’s': 5,\n",
              " 'win': 6,\n",
              " 'this': 7,\n",
              " 'team': 8,\n",
              " 'spirit': 9,\n",
              " 'cheer': 10,\n",
              " 'loud': 11,\n",
              " 'and': 12,\n",
              " 'proud': 13,\n",
              " 'champions': 14,\n",
              " 'never': 15,\n",
              " 'give': 16,\n",
              " 'up': 17,\n",
              " 'victory': 18,\n",
              " 'for': 19,\n",
              " 'the': 20,\n",
              " 'brave': 21,\n",
              " 'bumrah': 22,\n",
              " 'india': 23,\n",
              " 'on': 24,\n",
              " 'top': 25,\n",
              " 'unstoppable': 26,\n",
              " 'force': 27}"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(tokenizer.word_counts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EufpI3B7y4yb",
        "outputId": "3513946c-e21f-4d0a-b894-1a9ca86c1958"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "26"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.document_count"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FVf-B6fCy49-",
        "outputId": "f0203006-60f9-4a88-ccff-8ce8dff25906"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sequences = tokenizer.texts_to_sequences(docs)\n",
        "sequences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GV5co09Cy4-y",
        "outputId": "b7e3046a-672f-4d62-d2b0-fe8a464e81bc"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[5, 6, 7],\n",
              " [8, 9],\n",
              " [10, 11, 12, 13],\n",
              " [14, 15, 16, 17],\n",
              " [18, 19, 20, 21],\n",
              " [2, 2],\n",
              " [3, 3],\n",
              " [4, 4, 22],\n",
              " [23, 24, 25],\n",
              " [26, 27]]"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.utils import pad_sequences\n",
        "from keras.datasets import imdb\n",
        "from keras import Sequential\n",
        "from keras.layers import Dense,SimpleRNN,Embedding,Flatten"
      ],
      "metadata": {
        "id": "EUAZqaWty5C3"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sequences = pad_sequences(sequences,padding='post')\n",
        "sequences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cYRd5osiy5Dy",
        "outputId": "4f352a08-ee3a-480f-c09e-8506750d5ee1"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 5,  6,  7,  0],\n",
              "       [ 8,  9,  0,  0],\n",
              "       [10, 11, 12, 13],\n",
              "       [14, 15, 16, 17],\n",
              "       [18, 19, 20, 21],\n",
              "       [ 2,  2,  0,  0],\n",
              "       [ 3,  3,  0,  0],\n",
              "       [ 4,  4, 22,  0],\n",
              "       [23, 24, 25,  0],\n",
              "       [26, 27,  0,  0]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#this is just to show how the embedding will work and how the output will be, this will go as input to RNN\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(len(tokenizer.word_index) + 1,output_dim=2,input_length=4))\n",
        "\n",
        "#27 is the total vocabulary size of the data\n",
        "#2 is the size of the embedding that it outputs\n",
        "#input length is the length of the sentence(each sentence has token size of 4)\n",
        "\n",
        "model.compile('adam','accuracy')\n",
        "pred = model.predict(sequences)\n",
        "print(pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EuZbKXReT-va",
        "outputId": "7e466913-6725-4073-95c6-3b1980429599"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 118ms/step\n",
            "[[[-0.02114451 -0.00693255]\n",
            "  [ 0.00385623 -0.03393143]\n",
            "  [-0.01481849 -0.01782542]\n",
            "  [-0.01177373 -0.00139461]]\n",
            "\n",
            " [[-0.03098476 -0.0136076 ]\n",
            "  [ 0.04008241 -0.01832151]\n",
            "  [-0.01177373 -0.00139461]\n",
            "  [-0.01177373 -0.00139461]]\n",
            "\n",
            " [[-0.03977339  0.00536208]\n",
            "  [-0.0110826  -0.00837468]\n",
            "  [-0.02252886 -0.01368626]\n",
            "  [-0.02598659  0.04191611]]\n",
            "\n",
            " [[-0.03224351 -0.02935399]\n",
            "  [-0.0393079   0.00816391]\n",
            "  [-0.0496605   0.04118435]\n",
            "  [ 0.03418286  0.02109588]]\n",
            "\n",
            " [[ 0.00800309 -0.04100944]\n",
            "  [ 0.01515051  0.01618919]\n",
            "  [ 0.00054721  0.04782363]\n",
            "  [ 0.01212736 -0.01838906]]\n",
            "\n",
            " [[ 0.01981897  0.00486211]\n",
            "  [ 0.01981897  0.00486211]\n",
            "  [-0.01177373 -0.00139461]\n",
            "  [-0.01177373 -0.00139461]]\n",
            "\n",
            " [[ 0.03960109 -0.02920917]\n",
            "  [ 0.03960109 -0.02920917]\n",
            "  [-0.01177373 -0.00139461]\n",
            "  [-0.01177373 -0.00139461]]\n",
            "\n",
            " [[ 0.02686599 -0.01221006]\n",
            "  [ 0.02686599 -0.01221006]\n",
            "  [-0.01701404 -0.02254988]\n",
            "  [-0.01177373 -0.00139461]]\n",
            "\n",
            " [[ 0.01623711 -0.01874329]\n",
            "  [-0.00130374 -0.0090206 ]\n",
            "  [-0.00139391 -0.01008945]\n",
            "  [-0.01177373 -0.00139461]]\n",
            "\n",
            " [[-0.03318783  0.04112567]\n",
            "  [ 0.01898407  0.04913587]\n",
            "  [-0.01177373 -0.00139461]\n",
            "  [-0.01177373 -0.00139461]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.datasets import imdb\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.utils import pad_sequences\n",
        "from keras import Sequential\n",
        "from keras.layers import Dense,SimpleRNN,Embedding,Flatten"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kV6yaqt4T-xA",
        "outputId": "02bdd5a8-44c4-4907-cd1a-24ec6720cf36"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step\n",
            "[[[-0.04970398 -0.02842243]\n",
            "  [ 0.01249461 -0.04091763]\n",
            "  [-0.00894056 -0.04089009]\n",
            "  [-0.04170324  0.01762564]]\n",
            "\n",
            " [[-0.0414704  -0.0103724 ]\n",
            "  [ 0.03035598 -0.03492467]\n",
            "  [-0.04170324  0.01762564]\n",
            "  [-0.04170324  0.01762564]]\n",
            "\n",
            " [[ 0.02697888 -0.04915053]\n",
            "  [ 0.02185354 -0.01127043]\n",
            "  [-0.00476817 -0.03442337]\n",
            "  [ 0.01844556 -0.04728407]]\n",
            "\n",
            " [[-0.04058265  0.01705519]\n",
            "  [-0.0177679   0.04214079]\n",
            "  [-0.02351671 -0.03952732]\n",
            "  [-0.03313339 -0.03383388]]\n",
            "\n",
            " [[-0.02197774  0.04166773]\n",
            "  [ 0.036772    0.01115628]\n",
            "  [-0.03867519  0.00253033]\n",
            "  [-0.02192948  0.00768276]]\n",
            "\n",
            " [[-0.00666598  0.02584415]\n",
            "  [-0.00666598  0.02584415]\n",
            "  [-0.04170324  0.01762564]\n",
            "  [-0.04170324  0.01762564]]\n",
            "\n",
            " [[-0.03889466 -0.0460838 ]\n",
            "  [-0.03889466 -0.0460838 ]\n",
            "  [-0.04170324  0.01762564]\n",
            "  [-0.04170324  0.01762564]]\n",
            "\n",
            " [[-0.04697387  0.0449576 ]\n",
            "  [-0.04697387  0.0449576 ]\n",
            "  [-0.00925127  0.02469141]\n",
            "  [-0.04170324  0.01762564]]\n",
            "\n",
            " [[ 0.03155375  0.00807625]\n",
            "  [-0.03211845 -0.00949449]\n",
            "  [ 0.04261294  0.00376837]\n",
            "  [-0.04170324  0.01762564]]\n",
            "\n",
            " [[ 0.03434544  0.04419701]\n",
            "  [-0.02745148 -0.00810035]\n",
            "  [-0.04170324  0.01762564]\n",
            "  [-0.04170324  0.01762564]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(X_train,y_train),(X_test,y_test) = imdb.load_data(num_words=10000)"
      ],
      "metadata": {
        "id": "-88suMFHT_Ba"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = pad_sequences(X_train,padding='post',maxlen=50)\n",
        "X_test = pad_sequences(X_test,padding='post',maxlen=50)"
      ],
      "metadata": {
        "id": "9Abh8kQ0zOZn"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(10000, 2, input_length=50))\n",
        "model.add(SimpleRNN(32,return_sequences=False))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.summary()\n",
        "\n",
        "'''\n",
        "The Embedding layer in Keras does not use any predefined embedding method like Word2Vec, GloVe, or BERT by default. Instead, it uses:\n",
        "\n",
        "✅ Random initialization + learned training\n",
        "The Embedding layer starts with random weights.\n",
        "\n",
        "During model training, backpropagation updates the weights of the embedding matrix so that each token's vector becomes more useful for the task (e.g., classification, prediction).\n",
        "\n",
        "So, the process is:\n",
        "You provide integer-encoded tokens (e.g., [2, 4, 6, 1, 3]).\n",
        "\n",
        "The Embedding(input_dim=17, output_dim=2) layer creates a lookup table of shape (17, 2) — 17 tokens, each mapped to a 2D vector.\n",
        "\n",
        "Each input integer index is replaced by its corresponding 2D vector during the forward pass.\n",
        "\n",
        "These vectors are updated during training via gradient descent.\n",
        "\n",
        "Summary:\n",
        "Method used: Randomly initialized, learned embeddings via backpropagation.\n",
        "\n",
        "Not used: Pretrained embeddings (unless you explicitly load them)\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365
        },
        "id": "rP-ht2FwzOal",
        "outputId": "3009e81f-51f9-4ba8-c8d8-729734f262f0"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_17\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_17\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_12 (\u001b[38;5;33mEmbedding\u001b[0m)        │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ simple_rnn_12 (\u001b[38;5;33mSimpleRNN\u001b[0m)       │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_7 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)        │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ simple_rnn_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SimpleRNN</span>)       │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nThe Embedding layer in Keras does not use any predefined embedding method like Word2Vec, GloVe, or BERT by default. Instead, it uses:\\n\\n✅ Random initialization + learned training\\nThe Embedding layer starts with random weights.\\n\\nDuring model training, backpropagation updates the weights of the embedding matrix so that each token's vector becomes more useful for the task (e.g., classification, prediction).\\n\\nSo, the process is:\\nYou provide integer-encoded tokens (e.g., [2, 4, 6, 1, 3]).\\n\\nThe Embedding(input_dim=17, output_dim=2) layer creates a lookup table of shape (17, 2) — 17 tokens, each mapped to a 2D vector.\\n\\nEach input integer index is replaced by its corresponding 2D vector during the forward pass.\\n\\nThese vectors are updated during training via gradient descent.\\n\\nSummary:\\nMethod used: Randomly initialized, learned embeddings via backpropagation.\\n\\nNot used: Pretrained embeddings (unless you explicitly load them)\\n\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
        "history = model.fit(X_train, y_train,epochs=5,validation_data=(X_test,y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BwUEhFUQy5Qm",
        "outputId": "10e73dec-eb01-45a0-e41f-d13b111e028d"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 16ms/step - acc: 0.5237 - loss: 0.6905 - val_acc: 0.7539 - val_loss: 0.5094\n",
            "Epoch 2/5\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 16ms/step - acc: 0.7981 - loss: 0.4439 - val_acc: 0.8118 - val_loss: 0.4170\n",
            "Epoch 3/5\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 19ms/step - acc: 0.8713 - loss: 0.3172 - val_acc: 0.7995 - val_loss: 0.4424\n",
            "Epoch 4/5\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 19ms/step - acc: 0.8963 - loss: 0.2644 - val_acc: 0.7987 - val_loss: 0.4609\n",
            "Epoch 5/5\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 17ms/step - acc: 0.9172 - loss: 0.2233 - val_acc: 0.7960 - val_loss: 0.4996\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hgef7NFsZLbz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}